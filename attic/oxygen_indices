#!/usr/bin/python2.7

import argparse
import os
import logging
import json
import re
import subprocess

from o2lib import sign_file
from datetime import datetime

"""
    This looks at files in a location to create the index for Simple Streams.
    The output is similar to the following:

{
 "index": {
 "com.ubuntu.cloud:released:aws": {
    "updated": "Tue, 16 Apr 2013 08:13:19 +0000",
    "clouds": [
     {
       "region": "us-east-1",
       "endpoint": "http://ec2.us-east-1.amazonaws.com"
     } ],
    "cloudname": "aws",
    "datatype": "image-ids",
    "format": "products:1.0",
    "products": [
       "com.ubuntu.cloud:server:12.10:amd64",
    ],
      "path": "streams/v1/com.ubuntu.cloud:released:aws.js"
   },
   "com.ubuntu.cloud:released:download": {
     "datatype": "image-downloads",
     "path": "streams/v1/com.ubuntu.cloud:released:download.js",
     "updated": "Tue, 16 Apr 2013 08:12:28 +0000",
     "products": [
        "com.ubuntu.cloud:server:12.10:amd64",
     ],
   "format": "products:1.0"
   }
   },
 "updated": "Tue, 16 Apr 2013 08:13:19 +0000",
 "format": "index:1.0"
}

"""

# Get the location from the CLI
parser = argparse.ArgumentParser()
parser.add_argument("--base_d",
        action="store",
        required=True,
        help="Location to parse JSON and store index")
parser.add_argument("--gpg_key",
        action="store",
        required=False,
        default="7DB87C81",
        help="GPG Key ID for signing")
opts = parser.parse_args()

#Logging
logger = logging.getLogger('_indexgen_')
logging.basicConfig(format= \
            '%(asctime)s  %(levelname)s - %(message)s')
logger.setLevel(logging.DEBUG)

logger.info("Processing location %s" % opts.base_d)

# regex's to identify the file
json_re = re.compile('.*\.json$')
json_index_re = re.compile('index\.json$')
download_re = re.compile('.*download.*')

# Base index
index = {
    'updated': datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S +0000"),
    'format': 'index:1.0',
    'index': {},
    }

# Find the files and read them
for root, dirs, files in os.walk(opts.base_d):
    for name in files:
        name_f = (os.path.join(root, name))

        # Identify files for consideration
        if not json_re.match(name):
            logger.info("Skipping ineligible file %s" % name)
            continue

        if json_index_re.match(name):
            logger.info("Skipping %s as existing index" % name)
            continue

        logger.info("Found file %s" % name_f)
        logger.info(" - File is a candidate for consideration")

        # Read the file in
        file_data = None
        try:
            with open(name_f, 'r') as f:
                file_data = json.load(f)
            f.close()

            logger.info(" - Read json structure")

        except IOError as e:
            logger.warn(" - IOError reading file")
            logger.critical(e)
            continue

        except Exception as e:
            logger.warn(" - General error importing json")
            logger.critical(e)
            raise Exception("Probable corrupted JSON!")


        # Identify what the file describes
        is_cloud = True
        content_id = None
        anon = {}

        if 'format' in file_data:
            logger.info(" - Found stream %s" % file_data['format'])

        if 'content_id' in file_data:
            content_id = file_data['content_id']
            logger.info(" - Content is %s" % content_id)

        if not content_id:
            logger.warn(" - Unable to identify the content id. Skipping")
            continue

        if download_re.match(content_id):
            '''
                This block processes download indexes
            '''

            logger.info(" - Content is a file download index")

            anon = {
                'datatype': "image-downloads",
                'path': "streams/v1/%s" % name,
                'updated': file_data['updated'],
                'products': [],
                'format': "products:1.0",
                }

            for p_id in file_data['products']:
                logger.info("    - Found product %s" % p_id)
                anon['products'].append(p_id)

            ''' END download indexex '''

        else:
            '''
                This block processes cloud indexes
            '''

            logger.info(" - Content is a cloud publication index")

            anon = {
                'cloudname': content_id.split(':')[-1],
                'datatype': 'image-ids',
                'format': 'products:1.0',
                'products': [],
                'updated': file_data['updated'],
                'clouds': [],
                'path': "streams/v1/%s" % name,
                }

            logger.info(" - Cloud %s, %s" % (anon['cloudname'].upper(), anon['updated']))


            # Iterate over the product stream
            logger.info(" - Processing product identifications")
            for p_id in file_data['products']:
                logger.info("    - Found product %s" % p_id)
                anon['products'].append(p_id)

            # Iterate over the cloud endpoints
            logger.info(" - Processing cloud information")
            for crsn in file_data['_aliases']['crsn']:
                region = file_data['_aliases']['crsn'][crsn]
                logger.info("    - Found cloud/region %s" % crsn)

                _region = {
                        'region': region['region'],
                        'endpoint': region['endpoint'],
                    }

                anon['clouds'].append(_region)

            ''' END Cloud Indexing '''

        # If we get here, we're good to add it
        logger.info(" - Index key is %s" % content_id)
        if len(anon.keys()) > 0:
            index['index'][content_id] = anon
            logger.info(" - Added file content to index")
        else:
            logger.info(" - No items in index, skipping")

# Write the index
new_index = "%s/index.json" % opts.base_d
if 'streams/v1' not in opts.base_d:
    new_index = "%s/streams/v1/index.json" % opts.base_d

logger.info("Writing new index %s" % new_index)
with open(new_index, 'w') as f:
    json.dump(index, f, indent=1)
f.close()

## commented out. Instead of using this signing, we're using
## js2signed from simplestreams.
# Now sign the index
#if opts.gpg_key:
#    sign_file(new_index)
#    logger.info("Signed new index")

