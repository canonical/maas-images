#!/usr/bin/python3

import argparse
import copy
import os
import sys

from meph2 import util

from simplestreams import filters
from simplestreams import mirrors
from simplestreams import util as sutil
from simplestreams import objectstores

DEF_KEYRING = "/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg"

STREAMS_D = "streams/v1/"

LABELS = ('beta1', 'beta2', 'rc', 'release')

COMMON_ARGS = []
COMMON_FLAGS = {
    'dry-run': (('-n', '--dry-run'),
                {'help': 'only report what would be done',
                 'action': 'store_true', 'default': False}),
    'no-sign': (('-u', '--no-sign'),
                {'help': 'do not re-sign files',
                 'action': 'store_true', 'default': False}),
    'max': (('--max',),
            {'help': 'keep at most N items per product',
             'default': 1, 'type': int}),
    'orphan-data': (('orphan_data',), {'help': 'the orphan data file'}),
    'src': (('src',), {'help': 'the source streams directory'}),
    'target': (('target',), {'help': 'the target streams directory'}),
    'data_d': (('data_d',),
        {'help': 'the base data directory ("path"s are relative to this'}),
    'keyring': (('--keyring',),
        {'help': 'gpg keyring to check sjson', 'default': DEF_KEYRING}),
}

SUBCOMMANDS = {
    'insert': {
        'help': 'add new items from one stream into another',
        'opts': [
            COMMON_FLAGS['dry-run'], COMMON_FLAGS['no-sign'],
            COMMON_FLAGS['keyring'],
            COMMON_FLAGS['src'], COMMON_FLAGS['target'],
            ('filters', {'nargs': '*', 'default': []}),
        ]
    },
    'promote': {
        'help': 'promote a product/version from daily to release',
        'opts': [
            COMMON_FLAGS['dry-run'], COMMON_FLAGS['no-sign'],
            (('-l', '--label'),
             {'default': 'release', 'choices': LABELS,
              'help': 'the label to use'}),
            COMMON_FLAGS['src'], COMMON_FLAGS['target'],
            ('version', {'help': 'the version_id to promote.'}),
            ('filters', {'nargs': '+', 'default': []}),
        ]
    },
    'clean-md': {
        'help': 'clean streams metadata only to keep "max" items',
        'opts': [
            ('max', {'type': int}), ('target', {}),
            ('filters', {'nargs': '*', 'default': []}),
        ]
    },
    'find-orphans': {
        'help': 'find files in data_d not referenced in a "path"',
        'opts': [
            COMMON_FLAGS['orphan-data'], COMMON_FLAGS['data_d'],
            COMMON_FLAGS['keyring'],
            ('streams_dirs', {'nargs': '*', 'default': []}),
        ],
    },
    'reap-orphans': {
        'help': 'reap orphans listed in orphan-data from data_d',
        'opts': [
            COMMON_FLAGS['orphan-data'], COMMON_FLAGS['dry-run'],
            COMMON_FLAGS['data_d'], COMMON_FLAGS['keyring'],
            ('--older', {'default': '3d',
                'help': 'only remove files orphaned longer than'}),
        ],
    },
}


class BareMirrorWriter(mirrors.ObjectFilterMirror):
    # this does not do reference counting or .data/ storage
    # it stores data only in the streams
    def __init__(self, config, objectstore):
        super(BareMirrorWriter, self).__init__(config=config,
                                               objectstore=objectstore)
        self.store = objectstore
        self.config = config
        self.tproducts = None
        self.inserted = []

    def products_data_path(self, content_id):
        #base = "streams/v1/" + content_id
        #if os.path.exists(os.path.join(self.store.prefix, base + ".sjson")):
        #    return base + ".sjson"
        return "streams/v1/" + content_id + ".json"

    def load_products(self, path, content_id):
        sys.stderr.write("content_id=%s path=%s\n" % (content_id, path))
        ret = super(BareMirrorWriter, self).load_products(
            path=path, content_id=content_id)
        self.tproducts = copy.deepcopy(ret)
        return ret

    def insert_item(self, data, src, target, pedigree, contentsource):
        sys.stderr.write("inserting item %s\n" % '/'.join(pedigree))
        self.inserted.append(
            (pedigree, sutil.products_exdata(
                src, pedigree, include_top=False,
                insert_fieldnames=False)),)
        return super(BareMirrorWriter, self).insert_item(
            data, src, target, pedigree, contentsource)

    def insert_version(self, data, src, target, pedigree):
        sys.stderr.write("inserting version %s\n" % '/'.join(pedigree))
        return super(BareMirrorWriter, self).insert_version(data, src,
                                                            target, pedigree)

    def insert_products(self, path, target, content):
        sys.stderr.write("adding products %s\n" % path)

        ptouched = set([i[0][0] for i in self.inserted])
        srcitems = []

        def get_items(item, tree, pedigree):
            if pedigree[0] not in ptouched:
                return

            flat = sutil.products_exdata(tree, pedigree, include_top=False,
                                         insert_fieldnames=False)
            srcitems.append([pedigree, flat])

        sutil.walk_products(self.tproducts, cb_item=get_items)

        for pid in ptouched:
            oprod = self.tproducts['products'][pid]
            self.tproducts['products'][pid] = {}

        known_ints = ['size']
        for (pedigree, flatitem) in srcitems + self.inserted:
            for n in known_ints:
                if n in flatitem:
                    flatitem[n] = int(flatitem[n])
            sutil.products_set(self.tproducts, flatitem, pedigree)

        sutil.products_condense(self.tproducts,
                                sticky=['di_version', 'kpackage'])

        self.tproducts['updated'] = sutil.timestamp()

        ret = super(BareMirrorWriter, self).insert_products(
            path=path, target=self.tproducts, content=False)
        return ret

    def _noop(*args):
        return

    remove_item = _noop
    _inc_rc = _noop
    _dec_rc = _noop


class ReleasePromoteMirror(mirrors.ObjectFilterMirror):
    # this does not do reference counting or .data/ storage
    # it converts a daily item to a release item and inserts it.
    def __init__(self, config, objectstore, label):
        super(ReleasePromoteMirror, self).__init__(config=config,
                                               objectstore=objectstore)
        self.store = objectstore
        self.config = config
        self.tproducts = None
        self.inserted = []
        self.label = label

    def _noop(*args):
        return

    remove_item = _noop
    _inc_rc = _noop
    _dec_rc = _noop

    def rel2daily(self, ptree):
        ret = copy.deepcopy(ptree)
        ret['content_id'] = ret['content_id'].replace(":daily", "")

        for oname in [o for o in ptree.get('products', {})]:
            newname = oname.replace(".daily:", ":")
            ptree['products'][newname] = ptree['products'][oname]
            del ptree['products'][oname]

    def fixed_pedigree(self, pedigree):
        return tuple([pedigree[0].replace(".daily", "")] + list(pedigree[1:]))

    def products_data_path(self, content_id):
        #base = "streams/v1/" + content_id
        #if os.path.exists(os.path.join(self.store.prefix, base + ".sjson")):
        #    return base + ".sjson"
        return "streams/v1/" + content_id.replace(":daily", "") + ".json"

    def load_products(self, path, content_id):
        sys.stderr.write("content_id=%s path=%s\n" % (content_id, path))
        ret = super(ReleasePromoteMirror, self).load_products(
            path=path, content_id=content_id)
        self.tproducts = copy.deepcopy(ret)

        return self.rel2daily(ret)

    def insert_item(self, data, src, target, pedigree, contentsource):
        sys.stderr.write("inserting item %s\n" % '/'.join(pedigree))
        flat = sutil.products_exdata(src, pedigree, include_top=False,
                                     insert_fieldnames=False)
        flat['label'] = self.label
        self.inserted.append((self.fixed_pedigree(pedigree), flat),)
        return super(ReleasePromoteMirror, self).insert_item(
            data, src, target, pedigree, contentsource)

    def insert_version(self, data, src, target, pedigree):
        sys.stderr.write("inserting version %s\n" % '/'.join(pedigree))
        return super(ReleasePromoteMirror, self).insert_version(data, src,
                                                            target, pedigree)

    def insert_products(self, path, target, content):
        path = path.replace(":daily", "")
        sys.stderr.write("adding products %s\n" % path)

        ptouched = set([i[0][0] for i in self.inserted])
        srcitems = []

        def get_items(item, tree, pedigree):
            if pedigree[0] not in ptouched:
                return

            flat = sutil.products_exdata(tree, pedigree, include_top=False,
                                         insert_fieldnames=False)
            srcitems.append([self.fixed_pedigree(pedigree), flat])

        sutil.walk_products(self.tproducts, cb_item=get_items)

        for pid in ptouched:
            oprod = self.tproducts['products'][pid]
            self.tproducts['products'][pid] = {}

        known_ints = ['size']
        for (pedigree, flatitem) in srcitems + self.inserted:
            for n in known_ints:
                if n in flatitem:
                    flatitem[n] = int(flatitem[n])
            sutil.products_set(self.tproducts, flatitem, pedigree)

        sutil.products_condense(self.tproducts,
                                sticky=['di_version', 'kpackage'])

        self.tproducts['updated'] = sutil.timestamp()

        ret = super(ReleasePromoteMirror, self).insert_products(
            path=path, target=self.tproducts, content=False)
        return ret



def main_insert(args):
    (src_url, src_path) = sutil.path_from_mirror_url(args.src, None)
    filter_list = filters.get_filters(args.filters)

    def policy(content, path):
        if src_path.endswith('sjson'):
            return sutil.read_signed(content, keyring=args.keyring)
        else:
            return content

    mirror_config = {'max_items': 20, 'keep_items': True,
                     'filters': filter_list}
    smirror = mirrors.UrlMirrorReader(src_url, policy=policy)

    if args.dry_run:
        smirror = mirrors.UrlMirrorReader(src_url, policy=policy)
        tstore = objectstores.FileStore(args.target)
        drmirror = mirrors.DryRunMirrorWriter(config=mirror_config,
                                              objectstore=tstore)
        drmirror.sync(smirror, src_path)
        for (pedigree, path, size) in drmirror.downloading:
            fmt = "{pedigree} {path}"
            sys.stderr.write(
                fmt.format(pedigree='/'.join(pedigree), path=path) + "\n")
        return 0

    smirror = mirrors.UrlMirrorReader(src_url, policy=policy)
    tstore = objectstores.FileStore(args.target)
    tmirror = BareMirrorWriter(config=mirror_config, objectstore=tstore)
    tmirror.sync(smirror, src_path)

    md_d = os.path.join(args.target, "streams/v1/")
    util.create_index(md_d, files=None)

    if not args.no_sign:
        util.sign_streams_d(md_d)

    return 0


def main_promote(args):
    (src_url, src_path) = sutil.path_from_mirror_url(args.src, None)
    filter_list = filters.get_filters(args.filters)

    filter_list.extend(filters.get_filters(['version_name=%s' % args.version]))
    print("filter_list=%s" % filter_list)

    def policy(content, path):
        if src_path.endswith('sjson'):
            return sutil.read_signed(content, keyring=args.keyring)
        else:
            return content

    mirror_config = {'max_items': 100, 'keep_items': True,
                     'filters': filter_list}
    smirror = mirrors.UrlMirrorReader(src_url, policy=policy)

    if args.dry_run:
        smirror = mirrors.UrlMirrorReader(src_url, policy=policy)
        tstore = objectstores.FileStore(args.target)
        drmirror = mirrors.DryRunMirrorWriter(config=mirror_config,
                                              objectstore=tstore)
        drmirror.sync(smirror, src_path)
        for (pedigree, path, size) in drmirror.downloading:
            fmt = "{pedigree} {path}"
            sys.stderr.write(
                fmt.format(pedigree='/'.join(pedigree), path=path) + "\n")
        return 0

    smirror = mirrors.UrlMirrorReader(src_url, policy=policy)
    tstore = objectstores.FileStore(args.target)
    tmirror = ReleasePromoteMirror(config=mirror_config, objectstore=tstore,
                                   label=args.label)
    tmirror.sync(smirror, src_path)

    md_d = os.path.join(args.target, "streams/v1/")
    util.create_index(md_d, files=None)

    if not args.no_sign:
        util.sign_streams_d(md_d)

    return 0


def main_clean_md(args):
    raise NotImplementedError()


def main_find_orphans(args):
    data_d = args.data_d
    streams_d = args.streams_dirs
    streams_d.append(data_d)

    # used to check validity of existent orphan file at beginning
    if os.path.exists(args.orphan_data):
        _ = util.read_orphan_file(args.orphan_data)

    orphans = []

    non_orphans = util.get_nonorphan_set(streams_d, data_d, args.keyring)

    for (path, dirs, files) in os.walk(data_d):
        if os.path.join(path, '').startswith(
                os.path.join(data_d, 'streams', '')):
            continue
        if os.path.join(path, '').startswith(
                os.path.join(data_d, '.data', '')):
            continue

        for file_ in files:
            location = os.path.relpath(os.path.join(path, file_), data_d)
            if location not in non_orphans:
                orphans.append(location)

    util.write_orphan_file(args.orphan_data, orphans)
    return 0


def main_reap_orphans(args):
    data_d = args.data_d
    known_orphans = util.read_orphan_file(args.orphan_data)

    non_orphans = util.get_nonorphan_set([data_d], data_d, args.keyring)

    now = util.read_timestamp(sutil.timestamp())
    delta = util.read_timedelta(args.older)
    reaped = set()

    for orphan, when in known_orphans.items():
        location = os.path.join(data_d, orphan)
        if not util.read_timestamp(when) + delta < now:
            continue
        if location in non_orphans:
            raise Exception('Reap of %s failed: not an orphan' % orphan)
        if args.dry_run:
            sys.stderr.write('Reaping %s orphaned on %s\n' % (orphan, when))
        else:
            sutil.rm_f_file(location)
            reaped.add(orphan)
            try:
                os.removedirs(os.path.dirname(location))
            except:
                pass

    util.write_orphan_file(args.orphan_data, known_orphans.keys() - reaped)
    return 0


def main():
    parser = argparse.ArgumentParser()

    # Top level args
    for (args, kwargs) in COMMON_ARGS:
        parser.add_argument(*args, **kwargs)

    subparsers = parser.add_subparsers()
    for subcmd in sorted(SUBCOMMANDS.keys()):
        val = SUBCOMMANDS[subcmd]
        sparser = subparsers.add_parser(subcmd, help=val['help'])
        mfuncname = 'main_' + subcmd.replace('-', '_')
        sparser.set_defaults(action=globals()[mfuncname])
        for (args, kwargs) in val['opts']:
            if isinstance(args, str):
                args = [args]
            sparser.add_argument(*args, **kwargs)

    args = parser.parse_args()
    if not getattr(args, 'action', None):
        # http://bugs.python.org/issue16308
        parser.print_help()
        return 1

    return args.action(args)


if __name__ == '__main__':
    sys.exit(main())

# vi: ts=4 expandtab syntax=python
