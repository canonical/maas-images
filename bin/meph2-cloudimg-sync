#!/usr/bin/python3

from simplestreams import util as sutil
from simplestreams import contentsource
from simplestreams import log
from simplestreams.log import LOG
from simplestreams import mirrors
from simplestreams import filters

from meph2.netinst import NetbootMirrorReader

import argparse
import copy
import hashlib
import errno
import os
import subprocess
import sys
import tempfile
import yaml

CLOUD_IMAGES_DAILY = "http://cloud-images.ubuntu.com/daily/streams/v1/com.ubuntu.cloud:daily:download.json"
MAAS_EPHEM2_DAILY = "http://maas.ubuntu.com/images/ephemeral-v2/daily/streams/v1/com.ubuntu.maas:daily:v2:download.json"

DEFAULT_ARCHES = {
    'i386': ['i386'],
    'x86_64': ['i386', 'amd64', 'armhf'],
    'ppc64le': ['ppc64el'],
    'armhf': ['armhf'],
}

ALL_ITEM_TAGS = {'label': 'daily'}

CONTENT_ID = "com.ubuntu.maas.daily:v2:download"
PROD_PRE = "com.ubuntu.maas.daily:v2:boot"

PCOMMON = "%(release)s/%(arch)s/"
PATH_FORMATS = {
    'root-image.gz': PCOMMON + "%(version_name)s/root-image.gz",
    'boot-kernel': PCOMMON + "%(version_name)s/%(krel)s/%(flavor)s/boot-kernel",
    'boot-initrd': PCOMMON + "%(version_name)s/%(krel)s/%(flavor)s/boot-initrd",
    'di-initrd': PCOMMON + "di/%(di_version)s/%(krel)s/%(flavor)s/di-initrd",
    'di-kernel': PCOMMON + "di/%(di_version)s/%(krel)s/%(flavor)s/di-kernel",
}
PRODUCT_FORMAT = PROD_PRE + ":%(version)s:%(arch)s:%(psubarch)s"


def v2_to_cloudimg_products(prodtree):
    # this turns a v2 products tree into a cloud-image products tree.
    # it pays attention only to products with krel == release
    # (in an attempt to only get "primary")
    ret = empty_iid_products("com.ubuntu.cloud:daily:download")
    for product in prodtree.get('products'):
        if not (prodtree['products'][product].get('krel') ==
                prodtree['products'][product].get('release')):
            continue

        # com.ubuntu.maas:boot:12.04:amd64:hwe-s =>
        # com.ubuntu.cloud.daily:server:12.04:amd64
        tprod = ("com.ubuntu.cloud.daily:server:%(version)s:%(arch)s" %
                 prodtree['products'][product])

        if tprod not in ret['products']:
            ret['products'][tprod] = {'versions': {}}
        for vername in prodtree['products'][product].get('versions'):
            if vername not in ret['products'][tprod]['versions']:
                ret['products'][tprod]['versions'][vername] = {}

    return ret


def empty_iid_products(content_id):
    return {'content_id': content_id, 'products': {},
            'datatype': 'image-ids', 'format': 'products:1.0'}


def create_index(target_d, files=None, path_prefix="streams/v1/"):
    if files is None:
        files = [f for f in os.listdir(target_d) if f.endswith(".json")]

    ret = {'index': {}, 'format': 'index:1.0', 'updated': sutil.timestamp()}
    for f in files:
        with open(os.path.join(target_d, f), "r") as fp:
            data = sutil.load_content(fp.read())
        fmt = data.get('format')
        cid = data.get('content_id')
        if fmt == "index:1.0" or not (fmt and cid):
            continue
        optcopy = ('datatype', 'updated', 'format')
        item = {k: data.get(k) for k in optcopy if data.get(k)}
        if data.get('format') == "products:1.0":
            item['products'] = sorted([p for p in data['products'].keys()])

        item['path'] = path_prefix + f

        ret['index'][cid] = item

    return ret


def get_file_info(path, sums=None):
    buflen = 1024*1024

    if sums is None:
        sums = ['sha256']
    sumers = {k: hashlib.new(k) for k in sums}

    ret = {'size': os.path.getsize(path)}
    with open(path, "rb") as fp:
        while True:
            buf = fp.read(buflen)
            for sumer in sumers.values():
                sumer.update(buf)
            if len(buf) != buflen:
                break

    ret.update({k: sumers[k].hexdigest() for k in sumers})
    return ret


def get_di_kernelinfo(releases=None, arches=None, asof=None):
    # this returns a dict tree like
    # items['precise']['amd64']['generic']['saucy']['kernel']
    # where nodes are flattened to have data (including url)
    smirror = NetbootMirrorReader(releases=releases, arches=arches)
    netproducts = smirror._get_products()

    # TODO: implement 'asof' to get the right date, right now only returns
    # latest.

    items = {}
    tree_order = ('release', 'arch', 'kernel-flavor', 'kernel-release')

    def fillitems(item, tree, pedigree):
        flat = sutil.products_exdata(tree, pedigree)
        path = [flat[t] for t in tree_order]
        cur = items
        for tok in path:
            if tok not in cur:
                cur[tok] = {}
            cur = cur[tok]

        flat['url'] = smirror.source(item['path']).url
        ftype = flat['ftype']
        if (ftype not in cur or
                cur[ftype]['version_name'] < flat['version_name']):
            cur['di-' + ftype] = flat.copy()

    sutil.walk_products(netproducts, cb_item=fillitems)

    return (smirror, items)


def copy_fh(src, path, buflen=1024*8, cksums=None, makedirs=True):
    summer = sutil.checksummer(cksums)
    out_d = os.path.dirname(path)
    if makedirs:
        try:
            os.makedirs(out_d)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise
    tf = tempfile.NamedTemporaryFile(dir=out_d, delete=False)
    try:
        while True:
            buf = src.read(buflen)
            summer.update(buf)
            tf.write(buf)
            if len(buf) != buflen:
                break
    finally:
        if summer.check():
            try:
                os.rename(tf.name, path)
            except:
                os.unlink(tf.name)
                raise
        else:
            os.unlink(tf.name)
            raise ValueError("Invalid checksum")


class CloudImg2Meph2Sync(mirrors.BasicMirrorWriter):
    def __init__(self, config, out_d, target, v2config):
        super(CloudImg2Meph2Sync, self).__init__(config=config)
        self.out_d = out_d
        self.target = target
        self.v2config = v2config
        self.filters = self.config.get('filters', [])

        with open(v2config) as fp:
            cfgdata = yaml.load(fp)
        self.releases = {k['release']: k for k in cfgdata['releases']}
        arches = set()
        for r in self.releases.values():
            for k in r['kernels']:
                arches.add(k[1])
        self.arches = list(arches)
        self._di_kinfo = {}
        self.content_t = None

    def _get_di_kinfo(self, release, arch):
        if not (release in self._di_kinfo and
                arch in self._di_kinfo[release]):
            if release not in self._di_kinfo:
                self._di_kinfo[release] = {}
            (mirror, data) = get_di_kernelinfo(
                releases=[release], arches=[arch])
            self._di_kinfo[release][arch] = (mirror, data[release][arch])

        return self._di_kinfo[release][arch]

    def load_products(self, path=None, content_id=None):
        if content_id != "com.ubuntu.cloud:daily:download":
            raise ValueError("Not expecting to sync with content_id: %s" %
                             content_id)

        with contentsource.UrlContentSource(self.target) as tcs:
            my_prods = sutil.load_content(tcs.read())

        # need the list syntax to not update the dict in place
        for p in [p for p in my_prods['products']]:
            if "daily:v2" not in p:
                LOG.warn("skipping old product %s" % p)
                del(my_prods['products'][p])

        self.content_t = my_prods
        return v2_to_cloudimg_products(my_prods)

    def insert_item(self, data, src, target, pedigree, contentsource):
        flat = sutil.products_exdata(src, pedigree)

        # create the ephemeral root
        arch = flat['arch']
        release = flat['release']
        version = flat['version']

        # 'default_kernel' is a release dict in config
        # that contains the 'builtin' kernel to use
        dks = self.releases[flat['release']].get('default_kernel', {})
        builtin_kernel = dks.get(flat['arch'], 'linux-generic')

        (dimirror, dikinfo) = self._get_di_kinfo(release, arch)

        newitems = {}

        ikeys = ('boot-kernel', 'boot-initrd', 'di-initrd', 'di-kernel',
                 'root-image.gz')
        mykinfo = self.releases[release]['kernels']

        vername = flat['version_name']
        subs = {'release': release, 'arch': arch,
                'version_name': vername, 'version': version}

        rootimg_path = PATH_FORMATS['root-image.gz'] % subs

        gencmd = [os.environ.get('MAAS_CLOUDIMG2EPH2', "maas-cloudimg2eph2"),
                  "--kernel=%s" % builtin_kernel, "--arch=%s" % arch,
                  contentsource.url,
                  os.path.join(self.out_d, rootimg_path)]
        krd_packs = []
        newpaths = set((rootimg_path,))

        for (krel, karch, psubarch, flavor, kpkg, subarches) in mykinfo:
            if karch != arch:
                continue
            curdi = dikinfo[flavor][krel]
            subs.update({'krel': krel, 'kpkg': kpkg, 'flavor': flavor,
                         'psubarch': psubarch,
                         'di_version': curdi['di-kernel']['version_name']})

            prodname = PRODUCT_FORMAT % subs
            common = {'subarches': ','.join(subarches), 'krel': krel,
                      'release': release, 'version': version, 'arch': arch,
                      'subarch': psubarch, 'kflavor': flavor}
            common.update(ALL_ITEM_TAGS)

            items = {}
            for i in ikeys:
                items[i] = {'ftype': i, 'path': PATH_FORMATS[i] % subs,
                            'size': None, 'sha256': None}
                items[i].update(common)

            for key in ('di-kernel', 'di-initrd'):
                items[key]['sha256'] = curdi[key]['sha256']
                items[key]['size'] = int(curdi[key]['size'])
                items[key]['_opath'] = curdi[key]['path']

            krd_packs.append(
                (kpkg, os.path.join(self.out_d, items['boot-kernel']['path']),
                 os.path.join(self.out_d, items['boot-initrd']['path'])))
            newpaths.add(items['boot-kernel']['path'])
            newpaths.add(items['boot-initrd']['path'])

            newitems[prodname] = items

        for pack in krd_packs:
            gencmd.append('--krd-pack=' + ','.join(pack))

        if len([p for p in newpaths 
                if not os.path.exists(os.path.join(self.out_d, p))]) == 0:
            print("All paths existed, not re-generating: %s" % newpaths)
        else:
            print(gencmd)
            subprocess.check_call(gencmd)

        print(newpaths)
        # get checksum and size of new files created
        file_info = {}
        for path in newpaths:
            file_info[path] = get_file_info(os.path.join(self.out_d, path))

        for prodname in newitems:
            items = newitems[prodname]
            for item in items.values():
                item.update(file_info.get(item['path'], {}))

                lpath = os.path.join(self.out_d, item['path'])
                if ('_opath' in item and not os.path.exists(lpath)):
                    if not os.path.exists(os.path.dirname(lpath)):
                        os.makedirs(os.path.dirname(lpath))

                    try:
                        srcfd = dimirror.source(item['_opath'])
                        copy_fh(src=srcfd, path=lpath, cksums=item)
                    except ValueError:
                        raise ValueError("%s had bad checksum (%s)",
                                         srcfd.url, item['_opath'])
                    del(item['_opath'])

            for i in items:
                sutil.products_set(self.content_t, items[i],
                                   (prodname, vername, i))

    def insert_products(self, path, target, content):
        tree = copy.deepcopy(self.content_t)
        sutil.products_prune(tree)
        # stop these items from copying up when we call condense
        sutil.products_condense(tree)

        tsnow = sutil.timestamp()
        tree['updated'] = tsnow
        tree['datatype'] = 'image-downloads'

        dpath = "streams/v1/" + CONTENT_ID + ".json"
        fdpath = os.path.join(self.out_d, dpath)
        sdir = os.path.dirname(fdpath)
        LOG.info("writing data: %s", dpath)

        if not os.path.isdir(sdir):
            os.makedirs(sdir)

        with open(fdpath, "wb") as fp:
            fp.write(sutil.dump_data(tree))

        # now insert or update an index
        LOG.info("updating index in %s" % sdir)
        index = create_index(sdir)
        with open(os.path.join(sdir, "index.json"), "wb") as fp:
            fp.write(sutil.dump_data(index))

    def filter_index_entry(self, data, src, pedigree):
        if pedigree[0] != "com.ubuntu.cloud:daily:download":
            LOG.info("skipping index entry %s" % '/'.join(pedigree))
            return False
        return True

    def filter_product(self, data, src, target, pedigree):
        flat = sutil.products_exdata(src, pedigree)
        if flat['release'] not in self.releases:
            return False
        if flat['arch'] not in self.arches:
            return False
        return True

    def filter_item(self, data, src, target, pedigree):
        if data['ftype'] != "tar.gz":
            return False
        return filters.filter_item(self.filters, data, src, pedigree)


def main():
    defcfg = os.path.abspath(
        os.path.join(os.path.dirname(__file__),
                     "..", "conf", "meph-v2.yaml"))

    parser = argparse.ArgumentParser()

    parser.add_argument('--max', type=int, default=1,
                        help='store at most MAX items in the target')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='only report what would be done')
    parser.add_argument('--arches', action='append',
                        default=[], help='which arches to build, "," delim')
    parser.add_argument('--source', default=CLOUD_IMAGES_DAILY,
                        help='cloud images mirror')
    parser.add_argument('--target', default=MAAS_EPHEM2_DAILY,
                        help='maas ephemeral v2 mirror')
    parser.add_argument('--config', default=defcfg, help='v2 config')
    parser.add_argument('--verbose', '-v', action='count', default=0)
    parser.add_argument('--log-file', default=sys.stderr,
                        type=argparse.FileType('w'))

    parser.add_argument('output_d')
    parser.add_argument('filters', nargs='*', default=[])

    args = parser.parse_args()

    if len(args.arches) == 0:
        arches = DEFAULT_ARCHES[os.uname()[4]]
    else:
        arches = []
        for f in args.arches:
            arches.extend(f.split(","))

    arch_filter = "arch~(" + "|".join(arches) + ")"

    filter_list = filters.get_filters([arch_filter] + args.filters)

    (source_url, initial_path) = sutil.path_from_mirror_url(args.source, None)

    def policy(content, path):  # pylint: disable=W0613
        if initial_path.endswith('sjson'):
            return sutil.read_signed(content, keyring=args.keyring)
        else:
            return content

    mirror_config = {'max_items': args.max, 'filters': filter_list}

    level = (log.ERROR, log.INFO, log.DEBUG)[min(args.verbose, 2)]
    log.basicConfig(stream=args.log_file, level=level)

    smirror = mirrors.UrlMirrorReader(source_url, policy=policy)

    tmirror = CloudImg2Meph2Sync(config=mirror_config, out_d=args.output_d,
                                 target=args.target, v2config=args.config)

    tmirror.sync(smirror, initial_path)


if __name__ == '__main__':
    main()

# vi: ts=4 expandtab syntax=python
