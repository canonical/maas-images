#!/usr/bin/python3

from simplestreams import util as sutil
from simplestreams import contentsource
from simplestreams import log
from simplestreams.log import LOG
from simplestreams import objectstores
from simplestreams import mirrors
from simplestreams import filters

from meph2.netinst import NetbootMirrorReader

import argparse
import copy
import hashlib
import errno
import json
import os
import shutil
import subprocess
import sys
import tempfile
import yaml

CLOUD_IMAGES_DAILY = "http://cloud-images.ubuntu.com/daily/streams/v1/com.ubuntu.cloud:daily:download.json"
MAAS_EPHEM2_DAILY =  "http://maas.ubuntu.com/images/ephemeral-v2/daily/streams/v1/com.ubuntu.maas:daily:v2:download.json"

DEFAULT_ARCHES = {
    'i386': ['i386'],
    'x86_64': ['i386', 'amd64', 'armhf'],
    'ppc64le': ['ppc64el'],
    'armhf': ['armhf'],
}

CONTENT_ID = "com.ubuntu.maas.daily:v2:download"
PROD_PRE = "com.ubuntu.maas.daily:v2:boot"

PCOMMON = "%(release)s/%(arch)s/"
PATH_FORMATS = {
    'root-image.gz': PCOMMON + "%(version_name)s/root-image.gz",
    'boot-kernel': PCOMMON + "%(version_name)s/%(krel)s/%(flavor)s/boot-kernel",
    'boot-initrd': PCOMMON + "%(version_name)s/%(krel)s/%(flavor)s/boot-initrd",
    'di-initrd': PCOMMON + "di/%(di_version)s/%(krel)s/%(flavor)s/di-initrd",
    'di-kernel': PCOMMON + "di/%(di_version)s/%(krel)s/%(flavor)s/di-kernel",
}
PRODUCT_FORMAT = PROD_PRE + ":%(version)s:%(arch)s:%(psubarch)s"

def get_path(item, name, version):
    if item['ftype'] in ("root-image.tgz", "root-image.gz"):
        toks = (item['release'], item['arch'], version, name)
    elif item['ftype'] in ("di-kernel", "di-initrd"):
        toks = (item['release'], item['arch'], 'di', item['di-version'], item['krel'], item['kflavor'], name)
    elif item['ftype'] in ("boot-kernel", "boot-initrd"):
        toks = (item['release'], item['arch'], version, item['krel'], item['kflavor'], name)
    return '/'.join(toks)


def insert_item(item, fstore):
    src = contentsource.UrlContentSource(item['url'])
    fstore.insert(item['path'], contentsource.UrlContentSource(item['url']),
                  mutable=False, size=item['size'],
                  checksums={'sha256': item['sha256']})


def v2_to_cloudimg_products(prodtree):
    # this turns a v2 products tree into a cloud-image products tree.
    # it pays attention only to products with krel == release
    # (in an attempt to only get "primary")
    ret = empty_iid_products("com.ubuntu.cloud:daily:download")
    for product in prodtree.get('products'):
        if not (prodtree['products'][product].get('krel') ==
                prodtree['products'][product].get('release')):
            continue

        # com.ubuntu.maas:boot:12.04:amd64:hwe-s =>
        # com.ubuntu.cloud.daily:server:12.04:amd64
        tprod = ("com.ubuntu.cloud.daily:server:%(version)s:%(arch)s" %
                 prodtree['products'][product])
        
        if tprod not in ret['products']:
            ret['products'][tprod] = {'versions': {}}
        for vername in prodtree['products'][product].get('versions'):
            if vername not in ret['products'][tprod]['versions']:
                ret['products'][tprod]['versions'][vername] = {}
        
    return ret


def empty_iid_products(content_id):
    return {'content_id': content_id, 'products': {},
            'datatype': 'image-ids', 'format': 'products:1.0'}
    pass


def get_products(release, arch):
    data = None
    for rel, d in PRODUCTS_MAP.items():
        if d['release'] == release:
            data = d
    if data is None:
        return data

    products = {}
    for s in data['subarches']:
        tup = SUBARCHES[s]
        d = data.copy()
        d.update({'arch': tup[0], 'subarch': tup[1], 'kflavor': tup[2],
                  'krel': tup[3], 'subarches': tup[4]})
        if d['arch'] != arch:
            continue

        # if this is 'hwe-X' subarch, then support 'generic'
        if d['subarch'] == "hwe-" + release[0:1]:
            d['subarches'] = ['generic'] + d['subarches']

        d['subarches'] = ','.join(sorted(d['subarches']))

        pid = ':'.join([PROD_PRE, d['version'], d['arch'], d['subarch']])
        products[pid] = d
    return products

def insert_fake_item(item, fstore):
    content = item['path'] + "\n"
    item['size'] = len(content)
    fstore.insert_content(path=item['path'], content=content)
    item['_fake'] = 'fake-data: ' + item['path']


def read_sums(out_d):
    cfile = os.path.join(out_d, '.CHECKSUMS.json')
    if not os.path.isfile(cfile):
        return {}
    with open(cfile, "r") as fp:
        return json.load(fp)

def write_sums(out_d, sums=None):
    cfile = os.path.join(out_d, '.CHECKSUMS.json')
    if sums is None:
        return

    with open(cfile, "w") as fp:
        return json.dump(sums, fp)
    

def fill_item_from_fs(out_d, item):
    if not item.get('size'):
        item['size'] = os.path.getsize(os.path.join(out_d, item['path']))

    if not item.get('sha256'):
        sums = read_sums(out_d)
        cached = sums.get(item['path'])
        if cached:
            item['sha256'] = cached
        else:
            with open(os.path.join(out_d, item['path']), "r") as fp:
                buflen = 1024*1024
                cs = hashlib.new('sha256')
                while True:
                    buf = fp.read(buflen)
                    cs.update(buf)
                    if len(buf) != buflen:
                        break
            item['sha256'] = cs.hexdigest()
            sums[item['path']] = item['sha256']
            write_sums(out_d, sums)

    return


def get_eph_items(ephtgz, url, out_d, kernel, initrd):
    missing = []
    for i in (ephtgz, kernel, initrd):
       target = os.path.join(out_d, i['path'])
       if os.path.isfile(target):
           fill_item_from_fs(out_d, i)
       else:
           missing.append(i)

    if not missing:
        return

    mrootgz = ['./make-rootgz', url, out_d, ephtgz['path'],
               kernel['path'], initrd['path']]
    print("call: %s" % ' '.join(mrootgz))
    subprocess.check_call(mrootgz)
    for i in (ephtgz, kernel, initrd):
        fill_item_from_fs(out_d, i)
    return


def is_v1_included_kernel(arch, rel, krel, kflavor):
    if krel != rel:
        return False
    if (arch == "armhf" and kflavor == "highbank" and
        rel in ('precise', 'quantal', 'raring')):
            return True
    return (kflavor == "generic")


def empty_iid_products(content_id):
    return {'content_id': content_id, 'products': {},
            'datatype': 'image-ids', 'format': 'products:1.0'}


class V2mirror(mirrors.BasicMirrorWriter):

    def __init__(self, *args, **kwargs):
        super(V2mirror, self).__init__(config=kwargs.get('config'))
        self.out_d = kwargs.get('out_d')
        self.fstore = objectstores.FileStore(prefix=self.out_d)
        self.content_id = CONTENT_ID

    def _cidpath(self, content_id):
        return "streams/v1/%s.json" % content_id
    
    def load_products(self, path=None, content_id=None):
        if content_id != "com.ubuntu.maas.daily:download":
            raise ValueError("Not expecting to sync with content_id: %s" % content_id)

        try:
            path = self._cidpath(self.content_id)
            my_prods = sutil.load_content(self.fstore.source(path).read())
        except IOError as e:
            if e.errno != errno.ENOENT:
                raise
            my_prods = empty_iid_products(self.content_id)

        # we wont be updating the 'target' passed in, but rather this.
        self.content_t = my_prods
        return v2_to_cloudimg_products(my_prods)
        

    def insert_item(self, data, src, target, pedigree, contentsource):
        print("target=%s" % target)
        flat = sutil.products_exdata(src, pedigree)
        rel = flat['release']
        ver = flat['version']
        arch = flat['arch']
        version_name = flat['version_name']
        products = get_products(release=rel, arch=arch)
        #print "%s: %s" % (flat['product_name'], products.keys())
        ephtgz = {'ftype': 'root-image.gz',
                  'release': rel, 'arch': arch, 'version': ver}
        ephtgz['path'] = get_path(ephtgz, 'root-image.gz', version_name)
        eph_krds = []
        included = {}

        my_prods = self.content_t
        for pid, pdata in products.items():
            items = {}
            pdata['label'] = "daily"
            sutil.products_set(my_prods, pdata, (pid,))
            # get di-kernels/d-initrd
            common = {'release': rel, 'krel': pdata['krel'],
                      'arch': arch, 'subarch': pdata['subarch'],
                      'kflavor': pdata['kflavor'], 'version': ver}

            for item_name in ('di-kernel', 'di-initrd'):
                item = common.copy()
                item['ftype'] = item_name

                kinfo = get_krd_info(arch=arch, release=rel,
                                     krel=pdata['krel'], ftype=item_name,
                                     flavor=pdata['kflavor'])
                if kinfo is None:
                    raise Exception("No %s for %s / %s / %s" %
                                    (item_name, pid, arch, rel))
                item.update(kinfo)
               
                item['path'] = get_path(item, item_name, version_name)
                items[item_name] = item
                insert_item(item, self.fstore)
                sutil.products_set(my_prods, item, (pid, version_name, item_name,))

            for item_name in ('boot-kernel', 'boot-initrd'):
                item = common.copy()
                item['ftype'] = item_name
                item['path'] = get_path(item, item_name, version_name)
                items[item_name] = item

                if is_v1_included_kernel(arch, rel, pdata['krel'], pdata['kflavor']):
                    if included.get(item_name):
                        raise Exception("%s reset: %s => %s" %
                                        (item_name, included[item_name], item))
                    included[item_name] = item
                else:
                    insert_fake_item(item, self.fstore)
                    fill_item_from_fs(self.out_d, item)

                sutil.products_set(my_prods, item, (pid, version_name, item_name,))

                eph_krds.append(item)

            sutil.products_set(my_prods, ephtgz, (pid, version_name, 'root-image.gz',))

        if not included:
            raise Exception("No %s for %s / %s / %s" %
                            ('in_image_kernel', flat['product_name'], arch, rel))
        
        get_eph_items(ephtgz, contentsource.url, self.out_d,
                      included['boot-kernel'], included['boot-initrd'])
        #print data
        #print contentsource.url
            
    def insert_products(self, path, target, content):
        tree = copy.deepcopy(self.content_t)
        sutil.products_prune(tree)
        sticky = ['ftype', 'sha256', 'size', 'name', 'id', 'di-version']
        sutil.products_condense(tree, sticky)
        tsnow = sutil.timestamp()
        tree['updated'] = tsnow
        dpath = self._cidpath(tree['content_id'])

        print("writing to %s" % dpath)
        self.fstore.insert_content(dpath, sutil.dump_data(tree))


    def filter_product(self, data, src, target, pedigree):
        flat = sutil.products_exdata(src, pedigree)
        if flat['release'] in ('quantal', 'raring'):
            return False
        return True


def create_index(target_d, files=None, path_prefix="streams/v1/"):
    if files is None:
        files = [f for f in os.listdir(target_d) if f.endswith(".json")]

    ret = {'index': {}, 'format': 'index:1.0', 'updated': sutil.timestamp()}
    for f in files:
        with open(os.path.join(target_d, f), "r") as fp:
            data = sutil.load_content(fp.read())
        fmt = data.get('format')
        cid = data.get('content_id')
        if fmt == "index:1.0" or not (fmt and cid):
            continue
        optcopy = ('datatype', 'updated', 'format')
        item = {k: data.get(k) for k in optcopy if data.get(k)}
        if data.get('format') == "products:1.0":
            item['products'] = [p for p in data['products'].keys()]

        item['path'] = path_prefix + f

        ret['index'][cid] = item

    return ret


def get_file_info(path, sums=None):
    buflen = 1024*1024

    if sums is None:
        sums = ['sha256']
    sumers = {k: hashlib.new(k) for k in sums}

    ret = {'size': os.path.getsize(path)}
    with open(path, "rb") as fp:
        while True:
            buf = fp.read(buflen)
            for sumer in sumers.values():
                sumer.update(buf)
            if len(buf) != buflen:
                break

    ret.update({k: sumers[k].hexdigest() for k in sumers})
    return ret

def get_di_kernelinfo(releases=None, arches=None, asof=None):
    # this returns a dict tree like
    # items['precise']['amd64']['generic']['saucy']['kernel']
    # where nodes are flattened to have data (including url)
    smirror = NetbootMirrorReader(releases=releases, arches=arches)
    netproducts = smirror._get_products()

    # TODO: implement 'asof' to get the right date, right now only returns
    # latest.

    items = {}
    tree_order = ('release', 'arch', 'kernel-flavor', 'kernel-release')

    def fillitems(item, tree, pedigree):
        flat = sutil.products_exdata(tree, pedigree)
        path = [flat[t] for t in tree_order]
        cur = items
        for tok in path:
            if tok not in cur:
                cur[tok] = {}
            cur = cur[tok]

        flat['url'] = smirror.source(item['path']).url
        ftype = flat['ftype']
        if (ftype not in cur or
            cur[ftype]['version_name'] < flat['version_name']):
            cur['di-' + ftype] = flat.copy()


    sutil.walk_products(netproducts, cb_item=fillitems)

    return (smirror, items)


class CloudImg2Meph2Sync(mirrors.BasicMirrorWriter):
    def __init__(self, config, out_d, target, v2config):
        super(CloudImg2Meph2Sync, self).__init__(config=config)
        self.out_d = out_d
        self.target = target
        self.v2config = v2config
        self.filters = self.config.get('filters', [])

        with open(v2config) as fp:
            cfgdata = yaml.load(fp)
        self.releases = {k['release']: k for k in cfgdata['releases']}
        arches = set()
        for r in self.releases.values():
            for k in r['kernels']:
                arches.add(k[1])
        self.arches = list(arches)
        self._di_kinfo = {}
        self.content_t = None

    def _get_di_kinfo(self, release, arch):
        if not (release in self._di_kinfo and
                arch in self._di_kinfo[release]):
            if release not in self._di_kinfo:
                self._di_kinfo[release] = {}
            (mirror, data) = get_di_kernelinfo(
                releases=[release], arches=[arch])
            self._di_kinfo[release][arch] = (mirror, data[release][arch])

        return self._di_kinfo[release][arch]

    def load_products(self, path=None, content_id=None):
        if content_id != "com.ubuntu.cloud:daily:download":
            raise ValueError("Not expecting to sync with content_id: %s" % content_id)

        with contentsource.UrlContentSource(self.target) as tcs:
            my_prods = self.tmirror_products = sutil.load_content(tcs.read())

        self.content_t = my_prods
        return v2_to_cloudimg_products(my_prods)

    def filter_item(self, data, src, target, pedigree):
        if data['ftype'] != "tar.gz":
            return False
        return filters.filter_item(self.filters, data, src, pedigree)

    def insert_item(self, data, src, target, pedigree, contentsource):
        flat = sutil.products_exdata(src, pedigree)
        fields = ("release", "arch", "version_name", "ftype")

        # create the ephemeral root
        arch = flat['arch']
        release = flat['release']

        # 'default_kernel' is a release dict in config
        # that contains the 'builtin' kernel to use
        dks = self.releases[flat['release']].get('default_kernel', {})
        builtin_kernel = dks.get(flat['arch'], 'linux-generic')

        (dimirror, dikinfo) = self._get_di_kinfo(release, arch)

        newitems = {}
        
        ikeys = ('boot-kernel', 'boot-initrd', 'di-initrd', 'di-kernel',
                 'root-image.gz')
        mykinfo = self.releases[release]['kernels']

        vername = flat['version_name']
        subs = {'release': release, 'arch': arch,
                'version_name': vername, 'version': flat['version']}

        rootimg_path = PATH_FORMATS['root-image.gz'] % subs

        gencmd = ["maas-cloudimg2eph2",
                  "--kernel=%s" % builtin_kernel,
                  contentsource.url,
                  os.path.join(self.out_d, rootimg_path)]
        krd_packs = []
        newpaths = set((rootimg_path,))

        for (krel, karch, psubarch, flavor, kpkg, subarches) in mykinfo:
            if karch != arch:
                continue
            curdi = dikinfo[flavor][krel]
            subs.update({'krel': krel, 'kpkg': kpkg, 'flavor': flavor,
                         'psubarch': psubarch,
                         'di_version': curdi['di-kernel']['version_name']})

            prodname = PRODUCT_FORMAT % subs
            common = {'subarches': ','.join(subarches), 'krel': krel}

            items = {}
            for i in ikeys:
                items[i] = {'ftype': i, 'path': PATH_FORMATS[i] % subs,
                            'size': None, 'sha256': None}
                items[i].update(common)

            for key in ('di-kernel', 'di-initrd'):
                items[key]['sha256'] = curdi[key]['sha256']
                items[key]['size'] = int(curdi[key]['size'])
                items[key]['_opath'] = curdi[key]['path']

            krd_packs.append((kpkg,
                      os.path.join(self.out_d, items['boot-kernel']['path']),
                      os.path.join(self.out_d, items['boot-initrd']['path'])))
            newpaths.add(items['boot-kernel']['path'])
            newpaths.add(items['boot-initrd']['path'])

            newitems[prodname] = items

        for pack in krd_packs:
            gencmd.append('--krd-pack=' + ','.join(pack))
        print(gencmd)
        subprocess.check_call(gencmd)

        print(newpaths)
        # get checksum and size of new files created
        file_info = {}
        for path in newpaths:
            file_info[path] = get_file_info(os.path.join(self.out_d, path))

        for prodname in newitems:
            items = newitems[prodname]
            for item in items:
                item.update(file_info.get(item['path'], {}))
            sutil.products_set(self.content_t, items, [prodname, vername])

        import pprint; pprint.pprint(newitems)
        #cmd = [ 'gzip', '-9', '--rsyncable', rootimg_fp ]
        #print(' '.join(cmd))
            
#   kernel-release, arch, primary-subarch, flavor, subarches
        #print("/".join([flat[t] for t in fields]))

    def filter_index_entry(self, data, src, pedigree):
        prods = data.get('products', [])
    
        if pedigree[0] != "com.ubuntu.cloud:daily:download":
            LOG.info("skipping index entry %s" % '/'.join(pedigree))
            return False
        return True

    def filter_product(self, data, src, target, pedigree):
        flat = sutil.products_exdata(src, pedigree)
        if flat['release'] not in self.releases:
            return False
        if flat['arch'] not in self.arches:
            return False
        return True


def main():
    defcfg = os.path.abspath(
        os.path.join(os.path.dirname(__file__),
                     "..", "conf", "meph-v2.yaml"))

    parser = argparse.ArgumentParser()

    parser.add_argument('--max', type=int, default=1,
                        help='store at most MAX items in the target')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='only report what would be done')
    parser.add_argument('--arches', action='append',
                        default=[], help='which arches to build, "," delim')
    parser.add_argument('--source', default=CLOUD_IMAGES_DAILY,
                        help='cloud images mirror')
    parser.add_argument('--target', default=MAAS_EPHEM2_DAILY,
                        help='maas ephemeral v2 mirror')
    parser.add_argument('--config', default=defcfg, help='v2 config')
    parser.add_argument('--verbose', '-v', action='count', default=0)
    parser.add_argument('--log-file', default=sys.stderr,
                        type=argparse.FileType('w'))

    parser.add_argument('output_d')
    parser.add_argument('filters', nargs='*', default=[])

    args = parser.parse_args()

    if len(args.arches) == 0:
        arches = DEFAULT_ARCHES[os.uname()[4]]
    else:
        arches = []
        for f in args.arches:
            arches.extend(f.split(","))

    arch_filter = "arch~(" + "|".join(arches) + ")"

    filter_list = filters.get_filters([arch_filter] + args.filters)

    (source_url, initial_path) = sutil.path_from_mirror_url(args.source, None)

    def policy(content, path):  # pylint: disable=W0613
        if initial_path.endswith('sjson'):
            return sutil.read_signed(content, keyring=args.keyring)
        else:
            return content

    mirror_config = {'max_items': args.max, 'filters': filter_list}

    level = (log.ERROR, log.INFO, log.DEBUG)[min(args.verbose, 2)]
    log.basicConfig(stream=args.log_file, level=level)

    smirror = mirrors.UrlMirrorReader(source_url, policy=policy)

    tmirror = CloudImg2Meph2Sync(config=mirror_config, out_d=args.output_d,
                          target=args.target, v2config=args.config)

    tmirror.sync(smirror, initial_path)


if __name__ == '__main__':
    main()

# vi: ts=4 expandtab syntax=python

